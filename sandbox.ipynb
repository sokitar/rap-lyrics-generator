{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit ('.rapgen64': venv)",
   "metadata": {
    "interpreter": {
     "hash": "aed42f5a2742907e1b3b135efdb8a63da7f9c01c7dfca6623eac569acbda2999"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "#### Probar GPU\n",
    "Ver está activado el procesamiento con ayuda de la GPU"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "source": [
    "# SANDBOX\n",
    "Notebook para probar todo tipo de vainas\n",
    "## Prueba descarga líricas\n",
    "Ya teniendo el df con las URLs de las líricas, ver cómo descargar las canciones sin que haya fallos de letras vacías.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame, read_pickle\n",
    "from lyrics_scrapper import _URI_DF_CANCIONES, _NOMBRE_COL_ART,_NOMBRE_COL_LYR_URL, _NOMBRE_COL_IDS, _NOMBRE_COL_IDS, _NOMBRE_COL_LETRA,_URI_LETRAS\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "letras_df = read_pickle(_URI_DF_CANCIONES)\n",
    "artista = \"Dudu\"\n",
    "canart_df = letras_df[letras_df[_NOMBRE_COL_ART] == artista]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "' total_canciones = canart_df.shape[0] COMENTADO PARA QUE NO DESCARGUE SIEMPRE LAS CANCIONES DE DUDU QUE YA SE QUE FUNCIONA\\n\\nletras = [] # Lista que contendrá las letras de un artista\\nids_can = [] # Lista que contendrá los ids de las canciones\\n\\nn_can = 0\\nprint(f\"{n_can}/{total_canciones} de {artista} descargadas...\", end=\"\")\\nfor _, row in canart_df.iterrows():\\n\\n    try:\\n        page = requests.get(row[_NOMBRE_COL_LYR_URL])\\n    except requests.exceptions.RequestException:\\n        raise requests.exceptions.RequestException(f\"Ha ocurrido un error al descargar la lírica de la canción \"{row[_NOMBRE_COL_TIT]}\"...\")\\n\\n    html = BeautifulSoup(page.text, \"html.parser\")  # Extract the page\\'s HTML as a string\\n\\n    # Scrape the song lyrics from the HTML\\n    lyrics = html.find(\"div\", class_=\"lyrics\")\\n\\n    if lyrics is None:\\n        lyrics = html.find_all(\"div\", class_=re.compile(\"^Lyrics__Container\"))\\n        texto = \"\\n\".join([div.get_text() for div in lyrics])\\n    else:\\n        texto = lyrics.get_text()\\n\\n    if lyrics is None:\\n        letras_error+=1\\n        # print(f\"No se ha podido descargar la cancion {row[_NOMBRE_COL_TIT]} de {row[_NOMBRE_COL_ART]} [{row[_NOMBRE_COL_LYR_URL]}]...\")\\n        continue\\n\\n    letras.append(texto)\\n    ids_can.append(row[_NOMBRE_COL_IDS])\\n    n_can += 1\\n    print(f\"\\r{n_can}/{total_canciones} de {artista} descargadas...\", end=\"\")\\ndudu_df = DataFrame(data=np.array([ids_can, letras]).T, columns=[_NOMBRE_COL_IDS, _NOMBRE_COL_LETRA]) '"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "\"\"\" total_canciones = canart_df.shape[0] COMENTADO PARA QUE NO DESCARGUE SIEMPRE LAS CANCIONES DE DUDU QUE YA SE QUE FUNCIONA\n",
    "\n",
    "letras = [] # Lista que contendrá las letras de un artista\n",
    "ids_can = [] # Lista que contendrá los ids de las canciones\n",
    "\n",
    "n_can = 0\n",
    "print(f\"{n_can}/{total_canciones} de {artista} descargadas...\", end=\"\")\n",
    "for _, row in canart_df.iterrows():\n",
    "\n",
    "    try:\n",
    "        page = requests.get(row[_NOMBRE_COL_LYR_URL])\n",
    "    except requests.exceptions.RequestException:\n",
    "        raise requests.exceptions.RequestException(f\"Ha ocurrido un error al descargar la lírica de la canción \\\"{row[_NOMBRE_COL_TIT]}\\\"...\")\n",
    "\n",
    "    html = BeautifulSoup(page.text, \"html.parser\")  # Extract the page's HTML as a string\n",
    "\n",
    "    # Scrape the song lyrics from the HTML\n",
    "    lyrics = html.find(\"div\", class_=\"lyrics\")\n",
    "\n",
    "    if lyrics is None:\n",
    "        lyrics = html.find_all(\"div\", class_=re.compile(\"^Lyrics__Container\"))\n",
    "        texto = \"\\n\".join([div.get_text() for div in lyrics])\n",
    "    else:\n",
    "        texto = lyrics.get_text()\n",
    "\n",
    "    if lyrics is None:\n",
    "        letras_error+=1\n",
    "        # print(f\"No se ha podido descargar la cancion {row[_NOMBRE_COL_TIT]} de {row[_NOMBRE_COL_ART]} [{row[_NOMBRE_COL_LYR_URL]}]...\")\n",
    "        continue\n",
    "\n",
    "    letras.append(texto)\n",
    "    ids_can.append(row[_NOMBRE_COL_IDS])\n",
    "    n_can += 1\n",
    "    print(f\"\\r{n_can}/{total_canciones} de {artista} descargadas...\", end=\"\")\n",
    "dudu_df = DataFrame(data=np.array([ids_can, letras]).T, columns=[_NOMBRE_COL_IDS, _NOMBRE_COL_LETRA]) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dudu_df.iloc[7][_NOMBRE_COL_LETRA]"
   ]
  },
  {
   "source": [
    "## 2.- Preprocesamiento del texto\n",
    "Aquí miraré cómo es el texto y qué pasos hay que realizar para tokenizarlo. Analizaré una letra de C.Tangana, El Jincho y Natos y Waor respectivamente.\n",
    "### Visualización\n",
    "#### Letra Jincho"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jin = read_pickle(_URI_LETRAS + 'El Jincho.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sicariona yo soy un criminalPero tu eres más criminalLo mio es cristal baratoLo tuyo es originalYo con una cervezaTú con un litro de BrugalYo en el centro de menoresY tú presa en el penalYo en el hospitalTú en la clínica privadaYo con chandal del rastrilloTú con Louis Vuitton y pradaYo no voy al gymY tú tienes las tetas operadasYo tengo un macheteTú una prieta beretta cromadaYo doy bofetadasTú das botellazosY cuando meto manoTú quieres meter to' el brazoA mi me gusta pilaLo que a ti te mola mazoTú no eres de dar besitosNi yo soy de dar abrazos\n\nYo soy más de tarjetazosY tú de rayasYo que no hablo muchoY tú que nunca te callasYo soy Kinkillero NetoTú gitana a medias fallaYo hago freestyleY tú organizas todas las batallasYo quiero ir a las FallasTú a los San FerminesTú quieres una tarde locaY yo te llevo al cineMi abuelo recogía chatarraEl tuyo vendía kleenexTú chupas pa' que me corraYo chupo pa' que te orinesYo uso ListerineY tú te blanqueas los dientesYo soy antisocialY tú odias a la genteYo soy un perro viejoTú una zorra adolescenteYo estoy locoPero tu estás enferma de la mente\n\nYo tengo antecedentesTú un expediente terroristaTu naciste en ValdemingómezYo en OrcasitasYo soy mujeriegoY tú eres feministaYo me cuelo en el metroTú no pagas a los taxistasTú eres deportistaYo solo corro de los maderosYo tengo una BmxY tú tienes una ErosTú eres fan de MalúYo soy frustao' con el gueroYo no salgo del barrioTú viajas al extranjeroTu ex-novio es pandilleroY mi ex-novia es estudianteMis cadenas de plataY tu colgante de diamantesYo no quiero que tú bailesNi tú quieres que yo canteYo robo a camellosTú seduces narcotraficantes\n\nY es que tú eres más que yoEso se sabeTú tomas pastillasY yo jarabeA mi me trajo la cigüeñaY a ti dos marcianos en su naveYo podría seguir rapeandoPero tú quieres que acabeY es que túEres más que yoTú eres una sicarionaY yo soy un sicariónTú eres más sicaria que yoY eso que yo soy un sicariónY es que tú eres más que yoTú eres una sicarionaY yo soy un sicariónTú eres más sicaria que yoY eso que yo soy un sicariónAy, dale pa' lla' sicarionaTú eres más mata hombres que Pablo EscobarMás sicaria que Bin LadenY más psicopata mental que Star BurtonAsesinaDisaster estudiosLa J en los botocitosEl 28 de los mios personalesLa gente fuerte\n"
     ]
    }
   ],
   "source": [
    "can_jin = df_jin.iloc[4][_NOMBRE_COL_LETRA]\n",
    "print(can_jin)"
   ]
  },
  {
   "source": [
    "#### Letra C Tangana"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ctan = read_pickle(_URI_LETRAS + 'C Tangana.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n\n[Letra de \"Gente Normal\"]\n\n[Verse 1: C. Tangana]\nEstudiar y currar, dejar de pensar\nDetrás de la vida run and run and run\nMi rap me da igual, tu rap me da igual\nVacaciones aquí arriba, ganar y pagar\nGente normal, no falsos carceleros\nMis padres me lo dieron\nPrimero, hoy soy uno más\nPienso demasiado y ya está, ni morir ni matar\nTodos tenemos que dejar algo atrás\nVivir de las rentas no llena mi gas\nNo me llena de sus\nCaprichos humildes, los quiero pagar\nQuiero un diamante en su dedo, una casa en el mar\nPilla el sonido del bass, homenaje\nPara las calles con sudor de garaje\nTenía razón tu vieja, Fabi, cuando dijo\n\\\"descansar un rato hijo, caras enfermas\\\"\n\n- Manu, ¿que todo bien, como van las chispas?\n- Todo bien cabrón, paso a recogerte ¿A qué hora sales este finde?\n- Como a las tres, queda con la Inés, luego voy pa' donde estéis\n\n[Verse 2: Manto aka Sticky M.A.]\nPocos Jimi Hendrix y muchos Scarfaces\nHijos de puta ni os lo imagináis\nMis niños quieren droga, no quieren moda\nNada ni nadie que les joda\nQue esta mierda suba, reales y sencillos\nCurrando con mi viejo desde bien chiquillo\nY, apretando tornillos o dentro del tatami\nTirao encima de la cama escuchando a Big Daddy\nSúbelo Fabi, mantenlo puro\nY ¿Cuántas veces eche por patas con Guerudo?\nMariconas llamaron a la poli\nEspero el momento pa' que lo paguen caro\n¿Y verdad, Marcos, que hay mucha puta suelta?\nMirándote el culo cuando te das la vuelta\nMúsica y malta, Anna Kournikova en mis cartas\nEn Sants tapao con una manta\nAguanta, dispara como Eazy\nMi colega Chaiko pasando chinas con su bici\n¿Y tú que imbécil, de que fardas?\nMi peña normal, real, en mi espalda\n¿Verdad, mama?\nAprendimos el truco del pie en el suelo cuando íbamos ciego y daba vueltas la cama...la cama\nPor siempre E.M.S., Agorazeiners ¡Check it out!\n\n[Verse 3: C. Tangana]\nDespertar tarde, comer a deshora\nCon nueve dedos y mitad, a pachas las facturas\nCon la cabeza loca dándole más vueltas que la lavadora\nMañana llamo a la casera, ritmo en la escena\nPago en la vida, cuento lo que veo\nPorque me creo lo que digo y lo mantengo\nReal del cien al cero no te lo repito\nNo necesito convencerte así que callo\n¡Hey yo, hey yo!, la vena reventando el cuello\nCuando no hay fiche pa' el small todo son fallos\nNi por perro ni por viejo\nA veces se consigue lo mejor porque es lo necesario\nRafa que se jodan va por ti y por siempre\nCon siete hombres todos se hacen los valientes\nHay muchas plazas y mucho litros calientes\nMucho moderno con pose de delincuente\nCurrar, dejar de pensar, detrás de la vida, detrás de la vida\nTu rap me da igual, mi rap me da igual\nVacaciones aquí arriba, aquí arriba..\nGente normal, gente normal\nGente normal, gente normal...\nPeña de siempre, humilde, peña normal\n\n\n"
     ]
    }
   ],
   "source": [
    "can_ctan = df_ctan.iloc[9][_NOMBRE_COL_LETRA]\n",
    "print(can_ctan)"
   ]
  },
  {
   "source": [
    "#### Letra de Natos y Waor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nyw = read_pickle(_URI_LETRAS + 'Natos y Waor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Letra de \"Delirium\" ft. Recycled J[Verso 1: Waor]Mato la liga, tiento a la drogaMe dejo marcar sólo para jugar la prórrogaWhat's up, reina mora, cómo lo botasHazme de psicóloga, deshaz esta sogaYo, dinero negro y días grises por aquíTu madre no me quiere pa' ti, pero pa' ella síMuchos vacilan pero ¿Quién lo hace así?Bocas masticando aire, familias en crisisEse padre dando cera al SportiumLa madre llora en kelo hasta el ojete de ValiumDelirium. Su hijo no quiere estudiarSólo quiere bailar entre llamas en el pandemoniumCon los nervios de una yaya en el bingoY toa' la cara de pensar en mañana un puto domingoY otro día que despierto sin ser ricoMañana somos portada en los periódicos40 en el vaso, 90 en sus tetas26 a mis espaldas, el tiempo vuelaGiros de 180, -3 ahí fueraBilletes de 100 reflejados en mis córneasNúmeros 1 en esta mierdaEntro con los ojos tapaos en la curva a 3000 vueltasLa ruina, coca y apuestasSe paga 15 a 1 el que pasemos de los 30\n\n[Verso 2: Natos]Cada vez que discutimos, duelen más las cicatricesPerdóname otra vez por lo mal que lo hiceDejé de fumar, ahora bebo por quinceSalgo hecho un pincel, vuelvo viendo tripleYa ni bebo birra, sólo cubatasMe gusta el dinero, no la famaTrazar diagonales infinitas o volar en tu camaEntrar al hangar, correr por el túnel, saltar vallasMe echan del garito por entrar luciendo chándalY armarla, volcando fa en la barraOne, eight, seven y bandera a media asta¿Gangsta? Segurata cineastaAl chivato le cortaba la lenguaCórtate las venas, das pena, vergüenzaReza, reza, reza lo que sepasQue si el cable se me cruza, no queda uno con cabezaDe un tiempo a esta parte ya no sé quién es quiénGuess who's back in the game? Kurt Cocaine, chiki bang, bang!Me gustan los verdes, de cien en cien-como- Benjamin Franklin, -como- Wilt ChamberlainLo hago bien cabrón, porque así se haceEn el rap español, fiesta de disfracesEmpecé de abajo, nadie daba un duro por míY ahora matarían a su madre por tener mis frases\n\n[Puente: Recycled J]Cicatrices, kamikazes(Gesto serio y ropa de antes de ayer)Cicatrices, kamikazes(To' lo compra el parné, aún siento el hueco que dejaste)Cicatrices, kamikazes...Mis hélices en el desguaceFinales felices pero fugacesHumo, luces, Versace, Versace[Verso 3: Recycled J]Fifty en la \"massacre\"En Texas soy el RangerRecycled. Caer, levantarse en el parqueTodo por arte, Júpiter y MarteDale más, dale gas, lo bonito es estrellarseShorty, tengo por el mango la sarténLo haces bien, me he estudiao' tu papelHay cárceles que sólo existen en tu sienAll in, all in...Marcas en la piel, piel de camaleónCamuflao' en el humo entre mil cambios de humorVivo ArmaggedonJoven Dios en mi ciudadViral, crimi crimi criminalVeinte de alquitrán, orden para derribarHijos de la ruina-na-naMi piba desatendida como el GTAY yo matando la liga\n\n[Outro: Recycled J]Cicatrices, kamikazesChico easy, chica fácilFinales felices pero fugacesFinales felices pero...\n"
     ]
    }
   ],
   "source": [
    "can_nyw = df_nyw.iloc[13][_NOMBRE_COL_LETRA]\n",
    "print(can_nyw)"
   ]
  },
  {
   "source": [
    "#### Canción a eliminar.\n",
    "Canción que se deberá eliminar por ser demasiado corta."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can_corta = \"Hola soy una canción que se va a borrar porque no alcanzo la longitud suficiente.\""
   ]
  },
  {
   "source": [
    "### Limpieza de letras"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import re\n",
    "import string\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metemos todas las canciones en una lista de letras\n",
    "lets = []\n",
    "for f in glob.glob(\"./data/letras_raw/*\"):\n",
    "    lets += list(read_pickle(f)[_NOMBRE_COL_LETRA])"
   ]
  },
  {
   "source": [
    "#### Eliminar guión\n",
    "Eliminar los guiones que representan una conversación en la canción"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = []\n",
    "for letra in lets:\n",
    "    aux.append(letra.replace(\"- \", \"\"))\n",
    "lets = aux"
   ]
  },
  {
   "source": [
    "#### Eliminar título 1\n",
    "Algunas canciones empiezan con un título del estilo 'Letra de \"Delirium\" ft. Recycled J' que se sitúa antes de la primer corchete de apertura."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = []\n",
    "for letra in lets:\n",
    "    ind = letra.find(\"[\")\n",
    "    if ind != -1:\n",
    "        aux.append(letra[ind:])\n",
    "    else:\n",
    "        aux.append(letra)\n",
    "lets = aux"
   ]
  },
  {
   "source": [
    "#### Eliminar entradas entre corchetes y paréntesis.\n",
    "En algunas canciones se indica quién canta cada parte usando corchetes. Además de esto, se puede usar para indicar el título de la canción. También veces que en las letras salen frases hechas por voces secundarias. Estas las eliminaremos para simplificar el cuerpo."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = []\n",
    "for letra in lets:\n",
    "    aux.append(re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", letra))\n",
    "lets = aux"
   ]
  },
  {
   "source": [
    "#### Eliminar doble salto de línea"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = []\n",
    "for letra in lets:\n",
    "    l = re.sub(r'\\n+', '\\n', letra).strip()\n",
    "    aux.append(l)\n",
    "lets = aux"
   ]
  },
  {
   "source": [
    "#### Eliminar signos de puntuación\n",
    "\n",
    "Los signos de puntuación no aportan ninguna información semántica valiosa, por lo que los elimino. En modelos más complejos, se deberían conservar."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "puntuacion = string.punctuation + '¿¡' # Se añaden los signos de interrogación y exclamación invertidos ya que no los contiene string.punctuation\n",
    "aux = []\n",
    "for letra in lets:\n",
    "    l = letra.replace('-', ' ') # Sustituimos el guión por un espacio\n",
    "    for p in puntuacion:\n",
    "        l = l.replace(p,'')\n",
    "\n",
    "    l = re.compile(\"  +\").sub(' ', l)\n",
    "    aux.append(l)\n",
    "lets = aux"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 19,
   "outputs": []
  },
  {
   "source": [
    "#### Meter saltos de línea necesarios\n",
    "Algunos documentos se descargan de tal manera que no hay saltos de líneas salvo en los cambios de estrofa. Los saltos de línea hay que introducirse entre dos letras cuando una se trate de una letra minúscula seguida de una mayúscula"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = []\n",
    "for letra in lets:\n",
    "    ind = inserted = 0\n",
    "    if letra == '':\n",
    "        continue\n",
    "    while True:\n",
    "        if ind == len(letra)-1:\n",
    "            break\n",
    "        if letra[ind].islower(): # Vemos lsi la letra actual es minúcula\n",
    "            if letra[ind+1].isupper() or letra[ind+1].isnumeric(): # Si la siguiente letra es mayúscula, o un número, se introduce un salto de línea\n",
    "                # ind = ind + inserted\n",
    "                letra = letra[:ind+1] + \"\\n\" + letra[ind+1:]\n",
    "                inserted += 1\n",
    "        ind +=1\n",
    "\n",
    "    aux.append(letra) # Para eliminar los saltos de línea del principio\n",
    "lets = aux"
   ]
  },
  {
   "source": [
    "#### Cambiar todo a minúsculas\n",
    "[[Hay que hacerlo después de separar bien cada verso]] A la hora de tokenizar, cada palabra distinta se va a considerar un token individual. Por ello, para mejorar conservar toda la información semántica, transformo todo el texto a minúsculas, para que una palabra tenga asociado solo un token. (HAY UNA EXCEPCIÖN QUE SON LAS ABREVIACIONES como to')"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = []\n",
    "for letra in lets:\n",
    "    aux.append(letra.replace('\\n', ' \\n ')) # Para eliminar los saltos de línea del principio\n",
    "lets = aux"
   ]
  },
  {
   "source": [
    "#### Salto de línea como token\n",
    "En este apartado se procesarán las letras de tal manera que se entiendan los saltos de línea como token"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = []\n",
    "for letra in lets:\n",
    "    aux.append(letra.lower()) # Para eliminar los saltos de línea del principio\n",
    "lets = aux"
   ]
  },
  {
   "source": [
    "### Probar a crear un corpus\n",
    "Aquí se verá una pequeña muestra de cómo quedará el corpus"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_en_tokens = []\n",
    "for letra in lets:\n",
    "    texto_en_tokens += [w for w in letra.split(' ') if w.strip() != '' or w == '\\n']"
   ]
  },
  {
   "source": [
    "#### Cribar palabras poco frecuentes\n",
    "Dado que estamos haciendo la generación de texto por palabra, se va a crear un corpus muy grande. Con la finalidad de reducir la dimensionalidad de los datos y de reducir la carga en memoria y procesamiento, se eliminan las palabras que ocurren muy poco.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Unique words before ignoring: 51176\nIgnoring words with frequency < 2\nUnique words after ignoring: 26901\n"
     ]
    }
   ],
   "source": [
    "FREC_MINIMA_PALABRA = 2 # Límite inferior de veces que debe aparecer una palabra para ser considerada para la generación.\n",
    "frec_pala = {}\n",
    "for palab in texto_en_tokens:\n",
    "    frec_pala[palab] = frec_pala.get(palab, 0) + 1\n",
    "\n",
    "palabras_eliminadas = set()\n",
    "for k, v in frec_pala.items():\n",
    "    if frec_pala[k] < FREC_MINIMA_PALABRA:\n",
    "        palabras_eliminadas.add(k)\n",
    "\n",
    "\n",
    "palabras = set(texto_en_tokens)\n",
    "print('Unique words before ignoring:', len(palabras))\n",
    "print('Ignoring words with frequency <', FREC_MINIMA_PALABRA)\n",
    "palabras = sorted(set(palabras) - palabras_eliminadas)\n",
    "print('Unique words after ignoring:', len(palabras))"
   ]
  },
  {
   "source": [
    "#### Se crea unos diccionarios que relacionan palabras con índices en el corpus"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "palabra_idx = dict((c, i) for i, c in enumerate(palabras))\n",
    "idx_palabra = dict((i, c) for i, c in enumerate(palabras))"
   ]
  },
  {
   "source": [
    "#### Crear secuencias de palabras\n",
    "Se crean pares de secuencias de palabras, junto a la palabra que les sigue. Hay que tener en cuenta también las palabras que ya hemos ignorado. Se ignorarán los pares que contengan al menos una de las palabras ignoradas."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "SALTO = 1 # Número de palabras que se saltarán para crear la siguiente secuencia\n",
    "LONG_SEC = 10 # Longitud de la secuencia de palabras\n",
    "secuencias = [] # lista de secuencias\n",
    "sig_pal = [] # lista de siguientes palabras\n",
    "ign = 0 # Pares ignorados\n",
    "for i in range(0, len(texto_en_tokens) - LONG_SEC, SALTO):\n",
    "    # Only add sequences where no word is in ignored_words\n",
    "    if len(set(texto_en_tokens[i: i+LONG_SEC+1]).intersection(palabras_eliminadas)) == 0:\n",
    "        secuencias.append(texto_en_tokens[i: i + LONG_SEC])\n",
    "        sig_pal.append(texto_en_tokens[i + LONG_SEC])\n",
    "    else:\n",
    "        ign = ign+1\n",
    "print('Ignored sequences:', ign)\n",
    "print('Remaining sequences:', len(secuencias))"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 26,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Ignored sequences: 203390\nRemaining sequences: 875480\n"
     ]
    }
   ]
  },
  {
   "source": [
    "### Dividir en conjuntos de entrenamiento y test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tamaño set entrenamiento = 857970\nTamaño set prueba = 17510\n"
     ]
    }
   ],
   "source": [
    "def dividir_mezclar_set_entrenamiento(secuencias, sigs, porc_test=2):\n",
    "\n",
    "    sec_aux = []\n",
    "    sig_aux = []\n",
    "    for i in np.random.permutation(len(secuencias)):\n",
    "        sec_aux.append(secuencias[i])\n",
    "        sig_aux.append(sigs[i])\n",
    "\n",
    "    corte_idx = int(len(secuencias) * (1.-(porc_test/100.)))\n",
    "    x_entr, x_prue = sec_aux[:corte_idx], sec_aux[corte_idx:]\n",
    "    y_entr, y_prue = sig_aux[:corte_idx], sig_aux[corte_idx:]\n",
    "\n",
    "    print(\"Tamaño set entrenamiento = %d\" % len(x_entr))\n",
    "    print(\"Tamaño set prueba = %d\" % len(x_prue))\n",
    "    return x_entr, y_entr, x_prue, y_prue\n",
    "secuencias_entr, sig_pal_entr, secuencias_test, sig_pal_test = dividir_mezclar_set_entrenamiento(secuencias, sig_pal)"
   ]
  },
  {
   "source": [
    "### Crear modelo"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, LSTM, Bidirectional, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "LONG_SEC = 10 # Longitud de la secuencia de palabras\n",
    "DROPOUT=0.2\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(128), input_shape=(LONG_SEC, len(palabras))))\n",
    "if DROPOUT > 0:\n",
    "    model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(len(palabras)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])"
   ]
  },
  {
   "source": [
    "### Generador de datos\n",
    "Intentar suministrar todos los datos a la vez al modelo en la fase de entrenamiento puede llevar a errores de memoria. Por tanto, debemos codificar un generador de datos que haga un manejo eficiente de la memoria. De esta manera se generarán, iterativamente, batches de datos."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generador(lista_sec, lista_sigs, tam_batch):\n",
    "    idx = 0\n",
    "    while True:\n",
    "        x = np.zeros((tam_batch, LONG_SEC, len(palabras)), dtype=np.bool)\n",
    "        y = np.zeros((tam_batch, len(palabras)), dtype=np.bool)\n",
    "        for i in range(tam_batch):\n",
    "            for t, w in enumerate(lista_sec[idx]):\n",
    "                x[i, t, palabra_idx[w]] = 1\n",
    "            y[i, palabra_idx[lista_sigs[idx]]] = 1\n",
    "\n",
    "            idx = idx + 1\n",
    "            if idx == len(lista_sec):\n",
    "                idx = 0\n",
    "        yield x, y"
   ]
  },
  {
   "source": [
    "#### Entrenando el modelo"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import LambdaCallback, ModelCheckpoint, EarlyStopping\n",
    "# file_path = \"./checkpoints/LSTM_LYRICS-epoca{epoca:03d}-words%d-sequence%d-minfreq%d-loss{loss:.4f}-acc{acc:.4f}-val_loss{val_loss:.4f}-val_acc{val_acc:.4f}\" % (\n",
    "#     len(palabras),\n",
    "#     LONG_SEC,\n",
    "#     FREC_MINIMA_PALABRA\n",
    "# )\n",
    "# checkpoint = ModelCheckpoint(file_path, monitor='val_acc', save_best_only=True)\n",
    "# print_callback = LambdaCallback(on_epoch_end=on_epoch_end) #Pa esto se necesita la funcion on_epoch_end\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=5)\n",
    "# lista_callbacks = [checkpoint, print_callback, early_stopping]\n",
    "# lista_callbacks = [checkpoint, early_stopping]\n",
    "lista_callbacks = [early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n",
      " 1464/26812 [>.............................] - ETA: 17:34 - loss: 7.2659 - accuracy: 0.1144"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-99a424215a36>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mURL_MODELO\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# Si no existe ese modelo, crearlo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     model.fit_generator(generador(secuencias_entr, sig_pal_entr, TAM_BATCH),\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msecuencias_entr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mTAM_BATCH\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\UAM\\TFG\\rap-lyrics-generator\\.rapgen64\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1845\u001b[0m                   \u001b[1;34m'will be removed in a future version. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1846\u001b[0m                   'Please use `Model.fit`, which supports generators.')\n\u001b[1;32m-> 1847\u001b[1;33m     return self.fit(\n\u001b[0m\u001b[0;32m   1848\u001b[0m         \u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1849\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\UAM\\TFG\\rap-lyrics-generator\\.rapgen64\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1103\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[1;31m# No error, now safe to assign to logs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1105\u001b[1;33m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1106\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1107\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\UAM\\TFG\\rap-lyrics-generator\\.rapgen64\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    452\u001b[0m     \"\"\"\n\u001b[0;32m    453\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\UAM\\TFG\\rap-lyrics-generator\\.rapgen64\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    294\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unrecognized hook: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\UAM\\TFG\\rap-lyrics-generator\\.rapgen64\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    314\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\UAM\\TFG\\rap-lyrics-generator\\.rapgen64\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    354\u001b[0m       \u001b[0mhook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_supports_tf_logs'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 356\u001b[1;33m         \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    357\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Only convert once.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\UAM\\TFG\\rap-lyrics-generator\\.rapgen64\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1018\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1019\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1020\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1021\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1022\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\UAM\\TFG\\rap-lyrics-generator\\.rapgen64\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1082\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m       \u001b[1;31m# Only block async when verbose = 1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1084\u001b[1;33m       \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1085\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1086\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\UAM\\TFG\\rap-lyrics-generator\\.rapgen64\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    512\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 514\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\UAM\\TFG\\rap-lyrics-generator\\.rapgen64\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 659\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    661\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\UAM\\TFG\\rap-lyrics-generator\\.rapgen64\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 659\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    661\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\UAM\\TFG\\rap-lyrics-generator\\.rapgen64\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    508\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 510\u001b[1;33m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    511\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\UAM\\TFG\\rap-lyrics-generator\\.rapgen64\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1069\u001b[0m     \"\"\"\n\u001b[0;32m   1070\u001b[0m     \u001b[1;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1071\u001b[1;33m     \u001b[0mmaybe_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1072\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1073\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\UAM\\TFG\\rap-lyrics-generator\\.rapgen64\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1035\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1037\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1038\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m       \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from os.path import exists\n",
    "from keras.models import load_model\n",
    "\n",
    "TAM_BATCH = 32 # Tamaño de cada batch\n",
    "URL_MODELO = 'modelo.h5' # Nombre del archivo del modelo\n",
    "\n",
    "if not exists(URL_MODELO): # Si no existe ese modelo, crearlo\n",
    "    model.fit_generator(generador(secuencias_entr, sig_pal_entr, TAM_BATCH),\n",
    "        steps_per_epoch=int(len(secuencias_entr)/TAM_BATCH) + 1,\n",
    "        epochs=100,\n",
    "        callbacks=lista_callbacks,\n",
    "        validation_data=generador(secuencias_test, sig_pal_test, TAM_BATCH),\n",
    "        validation_steps=int(len(secuencias_test)/TAM_BATCH) + 1)\n",
    "    model.save(URL_MODELO) # Guardamos el modelo\n",
    "else:\n",
    "    model = load_model(URL_MODELO)"
   ]
  },
  {
   "source": [
    "#### Generando un resultado\n",
    "En esta sección se usará el modelo entrenado para generar una canción"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función utilitaria que samplea un índice de un array de probabilidades aplicando el sampleo por temperatura\n",
    "def sampleo(preds, temp=1.0):\n",
    "    preds = np.asarray(preds).astype('float64') # Cambiamos de tipo\n",
    "    preds = np.log(preds) / temp # Dividimos por temperatura, cuanto menor temperatura, mayor confianza tendrá el modelo en las probabilidades altas\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds) # Hacemos softmax\n",
    "    probas = np.random.multinomial(1, preds, 1) # TODO entender MULTINOMIAL\n",
    "    return np.argmax(probas)\n",
    "\n",
    "print(\"Textos de ejemplo\")\n",
    "\n",
    "# Se escoge una secuencia de inicio aleatoria.\n",
    "idx_semilla = np.random.randint(len(secuencias_entr+secuencias_test)) \n",
    "semilla = (secuencias_entr+secuencias_test)[idx_semilla]\n",
    "\n",
    "# Se crean textos con distinta diversidad (a más diversidad, menos similitud con textos originales)\n",
    "for diversidad in [0.1, 0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "    secuencia = semilla\n",
    "    print('----- Diversity:' + str(diversidad) + '\\n')\n",
    "    print('----- Generating with seed:\\n\"' + ' '.join(secuencia) + '\"\\n')\n",
    "    print(' '.join(secuencia), end='')\n",
    "\n",
    "    for i in range(50): #Se generaran 50 palabras\n",
    "        # Transformamos la secuencia de palabras en una matriz de entrada para modelo\n",
    "        x_pred = np.zeros((1, LONG_SEC, len(palabras)))\n",
    "        for t, palabra in enumerate(secuencia):\n",
    "            x_pred[0, t, palabra_idx[palabra]] = 1\n",
    "\n",
    "        # Hacemos la predicción\n",
    "        predicciones = model.predict(x_pred, verbose=0)[0]\n",
    "        \n",
    "        # Hacemos un sampleo sobre el vector de probabilidades que devuelve el modelo\n",
    "        sig_idx = sampleo(predicciones, diversidad)\n",
    "        sig_pala = idx_palabra[sig_idx]\n",
    "\n",
    "        secuencia = secuencia[1:]\n",
    "        secuencia.append(sig_pala)\n",
    "\n",
    "        print(\" \"+sig_pala, end=\"\")\n",
    "    print('\\n+++++++++++++++++++++++++++\\n')\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}